# -*- coding: utf-8 -*-
"""Sales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hXWVgqz2SOpxV89nsg3vIDrgsKYhIa4C

#### Problem Statement

You are provided with historical sales data for 45 stores of a Retail chain located in different
regions. Each store contains a number of departments, and you are tasked with predicting the
department-wide sales for each store.

#### The Sales Forecasting Problem

Sales forecasting is all about using historical data to inform decision making.


On its core, this is a [time series](https://en.wikipedia.org/wiki/Time_series#:~:text=A%20time%20series%20is%20a,sequence%20of%20discrete%2Dtime%20data.) problem: given some data in time, we want to predict the dynamics of that same data in the future. To do this, we require some trainable model of these dynamics.

##### Types of Forecasting Models

According to this [article](https://hbr.org/1971/07/how-to-choose-the-right-forecasting-technique) featured in the harvard business review, there are three types of Forecasting techniques:



- __Qualitative techniques__: usually involve expert opinion or information about special events.


- __Time series analysis and projection__: involve historical data,finding structure in the dynamics of the data like cyclical patterns, trends and growth rates.


    
- __Causal models__: these models involve the relevant causal relationships that may include pipeline considerations like inventories or market survey information. They can incorporate the results of a time series analysis.

I used the retail data analytics to test a simple auto regressive (AR) model to forecast the sales volume using sales only and sales + external information. While the data might be used to draw many insights on this retail business, here I focus only on implementing the AR model to forecast the sales volume.We will focus on the time series analysis approach which has been the driving force behind traditional forecasting methods and, it can give a comprehensive layout of the forecasting landscape.
"""

import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from datetime import datetime
import pandas as pd
import numpy as np
import plotly.express as px
import warnings
import seaborn as sns
warnings.filterwarnings("ignore")

features=pd.read_csv("features.csv")
features.head()

stores=pd.read_csv("stores.csv")
stores.head()

train=pd.read_csv("train.csv")
train.head()

test=pd.read_csv("test.csv")
test.head()

df=features.merge(stores,on='Store',how='outer')
df.head()

df=df.drop(['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'],axis=1)
df.head()

df.info()

from sklearn.preprocessing import LabelEncoder
l=LabelEncoder()
df['IsHoliday']=l.fit_transform(df['IsHoliday'])
df['Type']=l.fit_transform(df['Type'])

df.shape

df1=train.merge(df,on=['Date','Store'])
df1.head()

df1.info()

df1=df1.drop('IsHoliday_x',axis=1)

df1['Date'].nunique()

sales_weekly = df1.groupby(['Date', 'IsHoliday_y'], as_index = False).agg({'Weekly_Sales':"sum"})
sales_weekly

sales_weekly['IsHoliday_y'].value_counts()

df1['Weekly_Sales'].sum()

#temperature unit conversion
df1['Temperature'] = (df1['Temperature']- 32) * 5./9.

#factorize (Yes/No to 1/0 conversion)
holidays_factor, types =df1['IsHoliday_y'].factorize()
df1['IsHoliday_y'] = holidays_factor

df1['Date'] = df1['Date'].apply(pd.to_datetime)

df1.head()

df1['Y'] = df1['Date'].dt.year
df1['M'] = df1['Date'].dt.month
df1['D'] = df1['Date'].dt.dayofyear
df1['W'] = df1['Date'].dt.weekofyear

df1.head()

#### Sales analysis by dates, different types of stores and conditions
df_average_sales_weekly = df1.groupby('Date', as_index=False)\
    .agg({'Weekly_Sales': 'sum'})

df_average_sales_sorted = df_average_sales_weekly.sort_values('Weekly_Sales', ascending = False)

plt.figure(figsize=(20,5))

plt.plot(df_average_sales_weekly.Date, df_average_sales_weekly.Weekly_Sales, color = 'g')

plt.show()

### weekly sales mean by months
df_average_sales_monthly = df1.groupby('M', as_index=False)\
    .agg({'Weekly_Sales': 'mean'})
plt.figure(figsize=(20,5))
plt.plot(df_average_sales_monthly.M, df_average_sales_monthly.Weekly_Sales, color = 'r')
plt.show()

# Most profitable weeks
df_average_sales_sorted.head()

# Compare weekly sales by years
df_year10 = df1.query('Y == 2010').groupby('D', as_index=False)\
    .agg({'Weekly_Sales': 'sum'})
df_year11 = df1.query('Y == 2011').groupby('D', as_index=False)\
    .agg({'Weekly_Sales': 'sum'})
df_year12 = df1.query('Y == 2012').groupby('D', as_index=False)\
    .agg({'Weekly_Sales': 'sum'})
fig, ax = plt.subplots(figsize=(25,8))
ax.plot(df_year10.D, df_year10.Weekly_Sales, label = "2010")
ax.plot(df_year11.D, df_year11.Weekly_Sales, label = "2011")
ax.plot(df_year12.D, df_year12.Weekly_Sales, label = "2012")
ax.legend()
plt.show()

# Sales sums distribution

fig, ax = plt.subplots(figsize=(25,8))

df_year10['Weekly_Sales'].plot(kind='hist', title='Sales distribution', label = "2010");
df_year11['Weekly_Sales'].plot(kind='hist', label = "2011");
df_year12['Weekly_Sales'].plot(kind='hist', label = "2012");

ax.legend()
plt.show()

"""2012 sales do not have significant positive emissions like in 2010 and 2011 because of lack of data on 2012 december holidays weeks"""

# Rolling mean (window = 4)

df_average_sales_weekly['rol_month'] = df_average_sales_weekly['Weekly_Sales'].rolling(4).mean()

fig = plt.figure(figsize=(25,8))
line1, = plt.plot(df_average_sales_weekly.Date, df_average_sales_weekly.rol_month, '--', color='red')
line2, = plt.plot(df_average_sales_weekly.Date, df_average_sales_weekly.Weekly_Sales,  color='green')

fig.suptitle('Rolling mean - window = 4', fontsize=24)
plt.xlabel('Date', fontsize=18)
plt.ylabel('Sales', fontsize=16)
plt.legend((line2, line1), ['sum', 'rolling mean'])
plt.show()

"""The most profitable weeks and months coincide with the holidays of Christmas and Thanksgiving"""

# holiday weeks are marked with red lines
df2=df1.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()

f_1 = plt.figure(figsize=(12,6), dpi=100)
ax_1 = f_1.add_axes([0.0, 0.0, 0.9, 0.9])
ax_1.set_ylabel('Weekly_Sales')
ax_1.plot(df2['Date'], df2['Weekly_Sales'])

for x in df1[df1['IsHoliday_y']==1]['Date']:
    ax_1.axvline(x=x, color='red', linewidth=0.5)

"""##### Sales analysis by store types"""

df_type = df1.groupby('Type', as_index=False).agg(Mean=('Weekly_Sales', 'mean'), Sum=('Weekly_Sales', 'sum'))
df_type

plt.figure(figsize = (16,5))
ax = sns.barplot(x="Type", y="Weekly_Sales", hue="Y", data=df1, palette= "Paired")
ax.set_title('Mean weekly sales by years',fontsize=10)

df_group_type = df1.groupby('Type', as_index = False)\
    .agg({'Weekly_Sales':'mean'})\
    .sort_values('Weekly_Sales', ascending = False)
df_group_type

"""##### Top-5 stores by sales"""

df_group_store = df1.groupby('Store', as_index = False)\
    .agg({'Weekly_Sales':'sum'})\
    .sort_values('Weekly_Sales', ascending = False)
df_group_store.head()

"""##### Unemployment analysis"""

df_group_unemployment = df1.groupby('Y', as_index = False)\
    .agg({'Unemployment':'mean'})\
    .sort_values('Unemployment', ascending = False)
df_group_unemployment

"""##### The consumer price index"""

df_group_cpi = df1.groupby('Y', as_index = False)\
    .agg({'CPI':'mean'})\
    .sort_values('CPI', ascending = False)
df_group_cpi

"""##### Fuel Price tendency"""

df_average_sales_weekly2 = df1.groupby('Date', as_index=False)\
    .agg({'Fuel_Price': 'sum'})
df_average_sales_sorted = df_average_sales_weekly2.sort_values('Fuel_Price', ascending = False)

plt.figure(figsize=(20,5))
plt.plot(df_average_sales_weekly2.Date, df_average_sales_weekly2.Fuel_Price, color = 'orange')
plt.show()

"""##### Temperature analysis"""

labels=["less than -10 deg", "-10-0", "0-10", "10-15", "15-20", "20-25", "more than 25"]
bins=[-np.inf,-10,0,10,15,20,25, np.inf]
df1['temperature_category'] = pd.cut(df1['Temperature'], bins=bins, labels=labels,right=False)

df_group_temp = df1.groupby('temperature_category', as_index = False)\
    .agg({'Weekly_Sales':'sum'})\
    .sort_values('Weekly_Sales', ascending = False)
df_group_temp

plt.figure(figsize = (12,5))
ax = sns.countplot(x="temperature_category", data=df1, palette="coolwarm")
ax.set(xlabel="Category", ylabel = "Num", title="Sales")
vals = ax.get_yticks()

"""##### Feature correletion analysis for store 1"""

df_store1=df1.where( df1['Store'] == 1)
df_store1=df_store1.groupby(by=['Date'], as_index=False)[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 
                                                  'IsHoliday_y', 'Weekly_Sales']].mean()
df_store1 = df_store1.set_index('Date')
df_store1.head()

cor_store1=df_store1.corr()
cor_store1

plt.figure(figsize=(12,10))
sns.heatmap(cor_store1, annot=True)
plt.show()

"""#### Forecasting"""

import sklearn
from sklearn.model_selection import train_test_split
import cufflinks as cf
cf.go_offline()
cf.set_config_file(offline=False, world_readable=True)


import fbprophet
from fbprophet import Prophet

from fbprophet.diagnostics import cross_validation
from fbprophet.diagnostics import performance_metrics
from fbprophet.diagnostics import performance_metrics

from fbprophet.plot import add_changepoints_to_plot

import sys
from setuptools import setup

# make hidden imports explicit
from setuptools.command import bdist_egg, easy_install

#Data Preparation for Fbprophet

df_forecast=df1.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()

df_forecast = df_forecast.rename(columns = {'Date':'ds', 'Weekly_Sales':'y'})
df_forecast

# define the model
model = Prophet()
# fit the model
model.fit(df_forecast)

#Range of weeks for forecast
future_dates = pd.date_range("20121216","20141230", freq='W')

future_dates = [pd.to_datetime(i) for i in future_dates]

import datetime as dt
future = pd.DataFrame([dt.datetime.strftime(i, '%Y-%m-%d') for i in future_dates],  columns=['ds'])

forecast = model.predict(future)

print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())
# plot forecast
model.plot(forecast)
plt.show()

dates = pd.date_range("20111126","20121110", freq='W')
dates = [pd.to_datetime(i) for i in dates]
dates_for_forecast = pd.DataFrame([dt.datetime.strftime(i, '%Y-%m-%d') for i in dates],  columns=['ds'])

forecast_dates = model.predict(dates_for_forecast)
# summarize the forecast
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())
# plot forecast
model.plot(forecast_dates)
plt.plot(df_forecast.ds, df_forecast.y, color = 'orange')
plt.show()

"""

Take CPI, Temperature, IsHoliday as regressors for our model
"""

df_group_store1=df1.query('Store == 1')\
    .groupby(['Date', 'IsHoliday_y', 'CPI', 'Temperature'], as_index = False)\
    .agg({'Weekly_Sales':'sum'})
df_group_store1.head()

#Data Preparation for Fbprophet

df_group_store1 = df_group_store1.rename(columns = {'Date':'ds','Weekly_Sales':'y'})

datetime_series = pd.to_datetime(df_group_store1['ds'])

datetime_index = pd.DatetimeIndex(datetime_series.values)

Store1_data=df_group_store1.set_index(datetime_index)

Store1_data.head()

train_data_pr1 = Store1_data.iloc[:len(Store1_data)-40]
test_data_pr1 = Store1_data.iloc[len(Store1_data)-40:]
train_data_pr1.head()

test_data_pr1.head()

test_data_pr2 = test_data_pr1[['ds', 'IsHoliday_y', 'CPI', 'Temperature']]

# define the model
m1 = Prophet(changepoint_prior_scale=0.05, interval_width=0.95,\
             growth = 'linear',seasonality_mode = 'multiplicative', \
               yearly_seasonality=20, weekly_seasonality=True, changepoint_range=0.9)
m1.add_seasonality('weekly', period=7, fourier_order=15)

m1.add_regressor('IsHoliday_y')
m1.add_regressor('CPI')
m1.add_regressor('Temperature')
m1.fit(train_data_pr1)

prophet_pred = m1.predict(test_data_pr2)


print(prophet_pred[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())
# plot forecast
m1.plot(prophet_pred)

plt.plot(Store1_data.ds, Store1_data.y, color = 'orange')

plt.show()

m1.plot_components(prophet_pred)

Store1_data_1 = Store1_data[['ds', 'IsHoliday_y', 'CPI', 'Temperature']]

prophet_pred_1 = m1.predict(Store1_data_1)


print(prophet_pred_1[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())
# plot forecast
m1.plot(prophet_pred_1)


plt.plot(Store1_data.ds, Store1_data.y, color = 'orange')

for x in Store1_data[Store1_data['ds']=='2012-12-10']['ds']:
    plt.axvline(x=x, color='red', linewidth=0.5, ls='--', label='2012-12-10')
    

plt.legend()

plt.show()

prophet_pred_2 = prophet_pred_1[['ds','yhat']]
df_ds=pd.merge(Store1_data, prophet_pred_2, on=['ds'], how='left')
df_ds['diff'] = (df_ds['y']-df_ds['yhat'])/df_ds['y']

x1 = df_ds['ds']
y1 = df_ds['diff']
fig, ax = plt.subplots(figsize=(20,5))
ax.plot(x1.values, y1.values, c='r')

df_ds[['diff']].describe()

"""##### ARIMA forecast model

One of the common model used to forecast time series data is ARIMA. It stands for Autoregressive integrated moving average. One of the parameters are p, d & q. As you know the data has seasonality and let us use Seasonal ARIMA, SARIMAX to forecast the mode. There is a separate process to to identify the optmimum parameters, I did a grid search on GPU machine and it stopped after 700+ iterations.
"""

import statsmodels.api as sm
import warnings
warnings.filterwarnings("ignore")

weekly_sales = df1.groupby('Date')['Weekly_Sales'].mean().dropna()
mod = sm.tsa.statespace.SARIMAX(weekly_sales,
                                    order = (2, 0, 4),
                                    seasonal_order = (3, 1, 2, 12),
                                    enforce_stationarity = False,
                                    enforce_invertibility = True)
results = mod.fit()
results.plot_diagnostics(figsize=(15,12))
plt.show()

"""##### Static patterns

As the expression suggests, the concept of a static pattern relates to the idea of something that does not change. 

In Time Series, the most famous proxy for this concept is [__stationarity__](https://en.wikipedia.org/wiki/Stationary_process), which refers to the statistical properties of a time series that remain static: the observations in a stationary time series are not dependent on time.



___Time series are stationary if they do not have trend or seasonal effects.___ 

The trend and seasonality will affect the value of the time series at different times. Traditionally, we would be looking for consistency over time, for example by using the mean or the variance of the observations. When a time series is stationary, it can be easier to model and statistical modeling methods usually assume or require the time series to be stationary to be effective. 



If you want to dig deeper into stationarity I recommend this [piece](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322#:~:text=In%20the%20most%20intuitive%20sense,not%20itself%20change%20over%20time.) by @Shay Palachy.
"""

from statsmodels.tsa.stattools import adfuller

adf_test_sales = adfuller(list(df_store1["Weekly_Sales"]))
print("Retail sales results:")
print("ADF = " + str(adf_test_sales[0]))
print("p-value = " +str(adf_test_sales[1]))

"""#### Trends

A trend represents a tendency identified in our data. In a stock market scenario, this could be the trend of a given stock that appears to be going up or down. For Sales Forecasting, this is key: ___identifying a trend allows us to know the direction that our time-series is heading, which is fundamental for predicting the future of sales.___


We will use the [```fbprophet```](https://facebook.github.io/prophet/docs/quick_start.html) package to identify the overall trends for both our datasets. The steps will be:


- Select a range for the weather data (between 2007 and 2009)
- Feed the data to the ```fbprophet.Prophet``` object as a dataframe with two columns: "ds" (for date) and "y" (data) 

- Run the model
- Plot the trend with an upper and lower bound
"""

from fbprophet import Prophet


m = Prophet()
# Selecting one store
df_store_1=df_store1

df_store_1["Date"] = pd.to_datetime(df_store_1.index)
ds = df_store_1["Date"].dt.tz_localize(None)
y = df_store_1["Weekly_Sales"]
df_for_prophet = pd.DataFrame(dict(ds=ds,y=y))
m.fit(df_for_prophet)
future = m.make_future_dataframe(periods=15)
forecast = m.predict(future)
forecast = forecast[["ds","trend", "trend_lower", "trend_upper"]]
fig = m.plot_components(forecast,plot_cap=False)
trend_ax = fig.axes[0]
trend_ax.plot()
plt.title("Trend for Retail Data", fontsize=15)
plt.xlabel("Date", fontsize=15)
plt.ylabel("Sales Trend", fontsize=15)

plt.show()

"""##### Traditional Time Series Models to Sales Forecasting




So far, we covered the basics of the sales forecasting problem and identified the main components of it from a time series perspective: repeating patterns, static patterns and the idea of a trend. If you want to dig deeper on time series, I recommend this [article](https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a) by @Will Koehrsen.




Now we will look into the traditional time series approaches to deal with sales forecasting problems:




- Moving Average
- Exponential smoothing

- ARIMA

##### Moving Average
This model assumes that the next observation is the mean of all past observations and it can be used to identify interesting trends in the data. We can define a window to apply the moving average model to smooth the time series, and highlight different trends. 


Let's use the moving average model to predict the weather and sales. The steps will be:



- Select a range 
- Define a value for our moving average window
- Calculate the mean absolute error
- Plot an upper and lower bound for the rolling mean
- Plot the real data
"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

series = weekly_sales
window=15

rolling_mean = series.rolling(window=window).mean()
fig,ax = plt.subplots(figsize=(17,8))


plt.title('Moving Average Model for Retail Sales',fontsize=15)

plt.plot(rolling_mean, color= 'g', label='Rolling mean trend')

#Plot confidence intervals for smoothed values
mae = mean_absolute_error(series[window:], rolling_mean[window:])
deviation = np.std(series[window:] - rolling_mean[window:])
lower_bound = rolling_mean - (mae + 1.92 * deviation)
upper_bound = rolling_mean + (mae + 1.92 * deviation)

plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')
plt.plot(lower_bound, 'r--')

plt.plot(series[window:], color='b',label='Actual values')


plt.legend(loc='best')
plt.grid(True)
plt.xticks([])
plt.show()

"""##### Exponential Smoothing

Exponential smoothing is similar to moving average, but in this case a decreasing weight is assigned to each observation, so less importance is given to observations as we move further from the present. Such an assumption can be good and bad: it can be beneficial to decrease the weight of outdates information within the time-series dynamics, but it can be harmful when past information has some kind of permanent causal relationship with the dynamics of the data.  

Let's use exponential smoothing in the weather dataset used above, we will:
- Fit the data 
- Forecast 

- Plot the prediction agains the real values
"""

from statsmodels.tsa.api import ExponentialSmoothing
fit1 = ExponentialSmoothing(df_store1["Weekly_Sales"][0:200]).fit(smoothing_level=0.1, optimized=False)

fit2 = ExponentialSmoothing(df_store1["Weekly_Sales"][0:200]).fit(smoothing_level=0.5, optimized=False)

forecast1 = fit1.forecast(3).rename(r'$\alpha=0.1$')
forecast2 = fit2.forecast(3).rename(r'$\alpha=0.5$')
plt.figure(figsize=(17,8))

forecast1.plot(color='blue', legend=True)
forecast2.plot(color='red', legend=True)
df_store1["Weekly_Sales"][0:200].plot(marker='',color='green', legend=True)
plt.ylabel("Sales", fontsize=15)

fit1.fittedvalues.plot(color='blue')
fit2.fittedvalues.plot(color='red')

plt.title("Exponential Smoothing for Retail Data", fontsize=15)
plt.xticks([], minor=True)
plt.show()

"""##### Arima
ARIMA or Auto-regressive Integrated Moving Average is a time series model that aims to describe the auto-correlations in the time series data. It works well for short-term predictions and it can be useful to provide forecasted values for user-specified periods showing good results for demand, sales, planning, and production.

The parameters of the ARIMA model are defined as follows:

- p: The number of lag observations included in the model
- d: The number of times that the raw observations are differenced
- q: The size of the moving average window

Now I am going to use ARIMA model to model the weather data and retail sales. The steps will be:
- Split the data into training and testing
- Fit the data
- Print the mean square error (our evaluation metric)
- Plot the model fit with the real values

"""

from statsmodels.tsa.arima_model import ARIMA
from math import sqrt
from matplotlib import pyplot

X = df_store1["Weekly_Sales"].values

size = int(len(X) * 0.66)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]
predictions = list()
# walk-forward validation
for t in range(len(test)):
    model = ARIMA(history, order=(5,1,0))
    model_fit = model.fit()
    output = model_fit.forecast()
    yhat = output[0]
    predictions.append(yhat)
    obs = test[t]
    history.append(obs)
    print('predicted=%f, expected=%f' % (yhat, obs))
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()
print(model_fit.summary())

